# -*- coding: utf-8 -*-
"""Holiday_package_purchase_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUtYHTeZYbIOFk_A07WUg9dWA5c-OFm7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('Travel.csv')
df.head()

"""## **Data Cleaning**"""

df.isnull().sum()

df['Gender'].value_counts()

df['MaritalStatus'].value_counts()

df['TypeofContact'].value_counts()

df['Gender'] = df['Gender'].replace('Fe Male','Female')
df['MaritalStatus'] = df['MaritalStatus'].replace('Unmarried','Single')

df['Gender'].value_counts()

features_with_na = [features for features in df.columns if df[features].isnull().sum()>=1]
for feature in features_with_na:
    print(feature,np.round(df[feature].isnull().mean()*100,1),'% missing values')

df.Age.fillna(df.Age.mean(),inplace=True)
df.TypeofContact.fillna(df.TypeofContact.mode()[0],inplace=True)
df.DurationOfPitch.fillna(df.DurationOfPitch.mean(),inplace=True)
df.NumberOfTrips.fillna(df.NumberOfTrips.mode()[0],inplace=True)
df.NumberOfChildrenVisiting.fillna(df.NumberOfChildrenVisiting.mode()[0],inplace=True)
df.NumberOfFollowups.fillna(df.NumberOfFollowups.mode()[0],inplace=True)
df.MonthlyIncome.fillna(df.MonthlyIncome.mean(),inplace=True)

df.drop(['CustomerID'],axis=1,inplace=True)

"""## **Feature Engineering**"""

df['TotalVisiting'] = df['NumberOfChildrenVisiting'] + df['NumberOfPersonVisiting']
df.drop(['NumberOfChildrenVisiting','NumberOfPersonVisiting'],axis=1,inplace=True)

num_features = [feature for feature in df.columns if df[feature].dtype!='O']
print('Number of numerical features:',len(num_features))

cat_features = [feature for feature in df.columns if df[feature].dtype=='O']
print('Number of categorical features:',len(cat_features))

discrete_features = [feature for feature in num_features if len(df[feature].unique())<25]
print('Number of discrete features:',len(discrete_features))

continous_features = [feature for feature in num_features if len(df[feature].unique())>=25]
print('Number of continous features:',len(continous_features))

"""## **Train Test Split and Model Training**"""

from sklearn.model_selection import train_test_split
X = df.drop(['ProdTaken'],axis=1)
y = df['ProdTaken']

y.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape

cat_features = X.select_dtypes(include='object').columns.tolist()
num_features = X.select_dtypes(exclude='object').columns.tolist()

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

numerical_transformer = StandardScaler()
oh_transformer = OneHotEncoder(drop='first')

preprocessor = ColumnTransformer(
    transformers=[
        ('StandardScaler', numerical_transformer, num_features),
        ('OneHotEncoder', oh_transformer, cat_features)
    ])

preprocessor

X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

"""## **Random Forest Classifier Training**"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, r2_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, r2_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer

models = {
    "Random Forest" : RandomForestClassifier(),
    "Decsison Tree" : DecisionTreeClassifier(),
    "Logistic Regression" : LogisticRegression(),
    "KNN" : KNeighborsClassifier()
}

# Impute missing values after preprocessing
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)


for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train,y_train)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    model_train_accuracy = accuracy_score(y_train,y_train_pred)
    model_train_f1 = f1_score(y_train,y_train_pred)
    model_train_precision = precision_score(y_train,y_train_pred)
    model_train_recall = recall_score(y_train,y_train_pred)
    model_train_roc_auc = roc_auc_score(y_train,y_train_pred)

    model_test_accuracy = accuracy_score(y_test,y_test_pred)
    model_test_f1 = f1_score(y_test,y_test_pred)
    model_test_precision = precision_score(y_test,y_test_pred)
    model_test_recall = recall_score(y_test,y_test_pred)
    model_test_roc_auc = roc_auc_score(y_test,y_test_pred)


    print(list(models.keys())[i])

    print('Model performance for Training set')
    print("- Accuracy: {:.4f}".format(model_train_accuracy))
    print('- F1 Score: {:.4f}'.format(model_train_f1))
    print('- Precision: {:.4f}'.format(model_train_precision))
    print('- Recall: {:.4f}'.format(model_train_recall))
    print('- ROC AUC: {:.4f}'.format(model_train_roc_auc))

    print("______________________________________________")
    print()

    print('Model performance for Test set')
    print("- Accuracy: {:.4f}".format(model_test_accuracy))
    print('- F1 Score: {:.4f}'.format(model_test_f1))
    print('- Precision: {:.4f}'.format(model_test_precision))
    print('- Recall: {:.4f}'.format(model_test_recall))
    print('- ROC AUC: {:.4f}'.format(model_test_roc_auc))

    print('='*35)
    print('\n')

#Hyperparameter Tuning

rf_params = {
    'n_estimators': [100, 200, 300,1000],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'max_features' : ['auto', 'sqrt', 'log2']

}

randomcv_models = [
    ("RF", RandomForestClassifier(), rf_params)
]

randomcv_models

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, r2_score, roc_auc_score

model_param = {}
for name,model, params in randomcv_models:
    random = RandomizedSearchCV(estimator=model,param_distributions=params,n_iter=100,cv=5,verbose=2,n_jobs=-1)
    random.fit(X_train,y_train)
    model_param[name] = random.best_params_

for model_name in model_param:
    print(f"Best parameters for {model_name}: {model_param[model_name]}")

from sklearn.impute import SimpleImputer

models = {
    "Random Forest" : RandomForestClassifier(n_estimators=200, min_samples_split=2, max_features='sqrt', max_depth=20)
}

# Impute missing values after preprocessing
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)


for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train,y_train)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    model_train_accuracy = accuracy_score(y_train,y_train_pred)
    model_train_f1 = f1_score(y_train,y_train_pred)
    model_train_precision = precision_score(y_train,y_train_pred)
    model_train_recall = recall_score(y_train,y_train_pred)
    model_train_roc_auc = roc_auc_score(y_train,y_train_pred)

    model_test_accuracy = accuracy_score(y_test,y_test_pred)
    model_test_f1 = f1_score(y_test,y_test_pred)
    model_test_precision = precision_score(y_test,y_test_pred)
    model_test_recall = recall_score(y_test,y_test_pred)
    model_test_roc_auc = roc_auc_score(y_test,y_test_pred)


    print(list(models.keys())[i])

    print('Model performance for Training set')
    print("- Accuracy: {:.4f}".format(model_train_accuracy))
    print('- F1 Score: {:.4f}'.format(model_train_f1))
    print('- Precision: {:.4f}'.format(model_train_precision))
    print('- Recall: {:.4f}'.format(model_train_recall))
    print('- ROC AUC: {:.4f}'.format(model_train_roc_auc))

    print("______________________________________________")
    print()

    print('Model performance for Test set')
    print("- Accuracy: {:.4f}".format(model_test_accuracy))
    print('- F1 Score: {:.4f}'.format(model_test_f1))
    print('- Precision: {:.4f}'.format(model_test_precision))
    print('- Recall: {:.4f}'.format(model_test_recall))
    print('- ROC AUC: {:.4f}'.format(model_test_roc_auc))

    print('='*35)
    print('\n')

from sklearn.metrics import roc_auc_score, roc_curve
plt.figure()

auc_models = [
    {
        'label':'Random Forest Classifier',
        'model' : RandomForestClassifier(n_estimators=200, min_samples_split=2, max_features='sqrt', max_depth=20),
        'auc': 0.8313
    },

]

for algo in auc_models:
    model = algo['model']
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    plt.plot(fpr, tpr, label=f'{algo["label"]} (AUC = {algo["auc"]:.4f})')

plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')

